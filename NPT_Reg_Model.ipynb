{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\moham\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\moham\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\moham\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\keras\\src\\optimizers\\__init__.py:309: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "Epoch 1/2\n",
      "WARNING:tensorflow:From C:\\Users\\moham\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\moham\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\keras\\src\\engine\\base_layer_utils.py:384: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "1156/1156 [==============================] - 2707s 2s/step - loss: 1.0450 - accuracy: 0.9377 - val_loss: 0.4948 - val_accuracy: 0.9395\n",
      "Epoch 2/2\n",
      "1156/1156 [==============================] - 2590s 2s/step - loss: 0.4822 - accuracy: 0.9398 - val_loss: 0.4853 - val_accuracy: 0.9398\n",
      "322/322 [==============================] - 463s 1s/step - loss: 0.4858 - accuracy: 0.9397\n",
      "Test Accuracy: 0.9397284388542175\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, LSTM, Dense\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "# Load the dataset\n",
    "Reg_data = pd.read_csv(\"training_dataset.csv\")\n",
    "\n",
    "X_body = Reg_data['Body_tokens'].apply(lambda x: eval(x)).values\n",
    "X_title = Reg_data['Title_tokens'].apply(lambda x: eval(x)).values\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_body, X_title, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize Tokenizer and fit on the combined text\n",
    "Reg_tokenizer = Tokenizer()\n",
    "Reg_tokenizer.fit_on_texts(X_body)\n",
    "Reg_tokenizer.fit_on_texts(X_title)\n",
    "\n",
    "# Convert tokens to sequences of integers\n",
    "X_train_seq = Reg_tokenizer.texts_to_sequences(X_train)\n",
    "X_test_seq = Reg_tokenizer.texts_to_sequences(X_test)\n",
    "y_train_seq = Reg_tokenizer.texts_to_sequences(y_train)\n",
    "y_test_seq = Reg_tokenizer.texts_to_sequences(y_test)\n",
    "\n",
    "# Pad sequences to make them of equal length\n",
    "maxlen = 100  # Maximum sequence length\n",
    "X_train_pad = pad_sequences(X_train_seq, maxlen=maxlen, padding='post', truncating='post')\n",
    "X_test_pad = pad_sequences(X_test_seq, maxlen=maxlen, padding='post', truncating='post')\n",
    "y_train_pad = pad_sequences(y_train_seq, maxlen=maxlen, padding='post', truncating='post')\n",
    "y_test_pad = pad_sequences(y_test_seq, maxlen=maxlen, padding='post', truncating='post')\n",
    "\n",
    "# Define the LSTM model architecture\n",
    "embedding_dim = 100\n",
    "lstm_units = 64\n",
    "\n",
    "Reg_model = Sequential()\n",
    "Reg_model.add(Embedding(input_dim=len(Reg_tokenizer.word_index) + 1, output_dim=embedding_dim, input_length=maxlen))\n",
    "Reg_model.add(LSTM(units=lstm_units, dropout=0.2, recurrent_dropout=0.2, return_sequences=True))\n",
    "Reg_model.add(Dense(len(Reg_tokenizer.word_index) + 1, activation='softmax'))  # Predicting word indices\n",
    "\n",
    "# Compile the model\n",
    "Reg_model.compile(\n",
    "    optimizer='adam', \n",
    "    loss='sparse_categorical_crossentropy', \n",
    "    metrics=['accuracy']\n",
    "    )\n",
    "\n",
    "# Train the model\n",
    "batch_size = 32\n",
    "epochs = 2\n",
    "Reg_model.fit(X_train_pad, y_train_pad, batch_size=batch_size, epochs=epochs, validation_split=0.1)\n",
    "\n",
    "# Evaluate the model\n",
    "loss, accuracy = Reg_model.evaluate(X_test_pad, y_test_pad)\n",
    "print(\"Test Accuracy:\", accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2179  270   89  383 2458  609  904   52  586  123    2    1    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0]\n",
      "[  166   605   310 97560  1557  3647  1146    61   389   106   111    97\n",
      "    46     6   328   784   820   125  2916   111  3333   134   997   182\n",
      "     6  2916  1600   227  7720   703   783     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0]\n",
      "[270  89   2   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0]\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "Prediction 1 :\n",
      "Word: iphone, Probability: 0.0351\n",
      "Word: android, Probability: 0.0322\n",
      "Word: , Probability: 0.0596\n",
      "Word: , Probability: 0.1938\n",
      "Word: , Probability: 0.4665\n",
      "Word: , Probability: 0.7142\n",
      "Word: , Probability: 0.8686\n",
      "Word: , Probability: 0.9414\n",
      "Word: , Probability: 0.9725\n",
      "Word: , Probability: 0.9881\n",
      "Word: , Probability: 0.9954\n",
      "Word: , Probability: 0.9978\n",
      "Word: , Probability: 0.9985\n",
      "Word: , Probability: 0.9987\n",
      "Word: , Probability: 0.9988\n",
      "Word: , Probability: 0.9989\n",
      "Word: , Probability: 0.9989\n",
      "Word: , Probability: 0.9989\n",
      "Word: , Probability: 0.9989\n",
      "Word: , Probability: 0.9989\n",
      "Word: , Probability: 0.9989\n",
      "Word: , Probability: 0.9989\n",
      "Word: , Probability: 0.9989\n",
      "Word: , Probability: 0.9989\n",
      "Word: , Probability: 0.9989\n",
      "Word: , Probability: 0.9989\n",
      "Word: , Probability: 0.9989\n",
      "Word: , Probability: 0.9990\n",
      "Word: , Probability: 0.9990\n",
      "Word: , Probability: 0.9990\n",
      "Word: , Probability: 0.9990\n",
      "Word: , Probability: 0.9990\n",
      "Word: , Probability: 0.9990\n",
      "Word: , Probability: 0.9990\n",
      "Word: , Probability: 0.9990\n",
      "Word: , Probability: 0.9990\n",
      "Word: , Probability: 0.9990\n",
      "Word: , Probability: 0.9990\n",
      "Word: , Probability: 0.9990\n",
      "Word: , Probability: 0.9990\n",
      "Word: , Probability: 0.9990\n",
      "Word: , Probability: 0.9990\n",
      "Word: , Probability: 0.9990\n",
      "Word: , Probability: 0.9990\n",
      "Word: , Probability: 0.9990\n",
      "Word: , Probability: 0.9990\n",
      "Word: , Probability: 0.9990\n",
      "Word: , Probability: 0.9990\n",
      "Word: , Probability: 0.9990\n",
      "Word: , Probability: 0.9990\n",
      "Word: , Probability: 0.9990\n",
      "Word: , Probability: 0.9990\n",
      "Word: , Probability: 0.9990\n",
      "Word: , Probability: 0.9990\n",
      "Word: , Probability: 0.9990\n",
      "Word: , Probability: 0.9990\n",
      "Word: , Probability: 0.9990\n",
      "Word: , Probability: 0.9990\n",
      "Word: , Probability: 0.9990\n",
      "Word: , Probability: 0.9990\n",
      "Word: , Probability: 0.9990\n",
      "Word: , Probability: 0.9990\n",
      "Word: , Probability: 0.9990\n",
      "Word: , Probability: 0.9990\n",
      "Word: , Probability: 0.9990\n",
      "Word: , Probability: 0.9990\n",
      "Word: , Probability: 0.9990\n",
      "Word: , Probability: 0.9990\n",
      "Word: , Probability: 0.9990\n",
      "Word: , Probability: 0.9990\n",
      "Word: , Probability: 0.9990\n",
      "Word: , Probability: 0.9990\n",
      "Word: , Probability: 0.9990\n",
      "Word: , Probability: 0.9990\n",
      "Word: , Probability: 0.9990\n",
      "Word: , Probability: 0.9990\n",
      "Word: , Probability: 0.9990\n",
      "Word: , Probability: 0.9990\n",
      "Word: , Probability: 0.9990\n",
      "Word: , Probability: 0.9990\n",
      "Word: , Probability: 0.9990\n",
      "Word: , Probability: 0.9990\n",
      "Word: , Probability: 0.9990\n",
      "Word: , Probability: 0.9990\n",
      "Word: , Probability: 0.9990\n",
      "Word: , Probability: 0.9990\n",
      "Word: , Probability: 0.9990\n",
      "Word: , Probability: 0.9990\n",
      "Word: , Probability: 0.9990\n",
      "Word: , Probability: 0.9990\n",
      "Word: , Probability: 0.9990\n",
      "Word: , Probability: 0.9990\n",
      "Word: , Probability: 0.9990\n",
      "Word: , Probability: 0.9990\n",
      "Word: , Probability: 0.9990\n",
      "Word: , Probability: 0.9990\n",
      "Word: , Probability: 0.9990\n",
      "Word: , Probability: 0.9990\n",
      "Word: , Probability: 0.9990\n",
      "Word: , Probability: 0.9990\n",
      "\n",
      "Predicted Title: iphone android\n"
     ]
    }
   ],
   "source": [
    "# Prompt the user to enter their input\n",
    "user_input = input(\"Enter the body text: \")\n",
    "\n",
    "user_input_seq = Reg_tokenizer.texts_to_sequences([user_input])\n",
    "user_input_pad = pad_sequences(user_input_seq, maxlen=maxlen, padding='post', truncating='post')\n",
    "\n",
    "# Predict the title based on the user input\n",
    "predicted_title_probs = Reg_model.predict(user_input_pad)\n",
    "\n",
    "# Find the index of the word with the highest probability for each prediction\n",
    "predicted_title_indices = predicted_title_probs.argmax(axis=2)\n",
    "\n",
    "# Get the probabilities associated with each word in the vocabulary\n",
    "for i, indices in enumerate(predicted_title_indices):\n",
    "    print(\"Prediction\", i+1, \":\")\n",
    "    for j, index in enumerate(indices):\n",
    "        probability = predicted_title_probs[i, j, index]\n",
    "        word = Reg_tokenizer.sequences_to_texts([[index]])[0]\n",
    "        print(f\"Word: {word}, Probability: {probability:.4f}\")\n",
    "    print()\n",
    "\n",
    "predicted_title = Reg_tokenizer.sequences_to_texts(predicted_title_indices)[0]\n",
    "\n",
    "print(\"Predicted Title:\", predicted_title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  8/322 [..............................] - ETA: 1:01:04"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Get predictions\n",
    "predictions = Reg_model.predict(X_test_pad)\n",
    "y_pred = (predictions > 0.5).astype(int)\n",
    "\n",
    "# Calculate confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# Plot confusion matrix\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False)\n",
    "plt.xlabel('Predicted labels')\n",
    "plt.ylabel('True labels')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
