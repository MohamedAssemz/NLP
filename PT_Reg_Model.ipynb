{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>Title</th>\n",
       "      <th>Body</th>\n",
       "      <th>Score</th>\n",
       "      <th>ViewCount</th>\n",
       "      <th>Label</th>\n",
       "      <th>LabelNum</th>\n",
       "      <th>Title_Length</th>\n",
       "      <th>Body_Length</th>\n",
       "      <th>Title_Sentiment</th>\n",
       "      <th>Title_tokens</th>\n",
       "      <th>Body_tokens</th>\n",
       "      <th>Title_clean</th>\n",
       "      <th>Body_clean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>197234</td>\n",
       "      <td>drop\\stop mobile data connection (non-wifi) by...</td>\n",
       "      <td>&lt;p&gt;can i set android 4.4.2 to drop mobile data...</td>\n",
       "      <td>0</td>\n",
       "      <td>34</td>\n",
       "      <td>android</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>34</td>\n",
       "      <td>0.000</td>\n",
       "      <td>['drop', 'stop', 'mobile', 'data', 'connection...</td>\n",
       "      <td>['set', 'android', '4.4.2', 'drop', 'mobile', ...</td>\n",
       "      <td>drop stop mobile data connection non-wifi cond...</td>\n",
       "      <td>set android 4.4.2 drop mobile data connection ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>114800</td>\n",
       "      <td>how to automatically crop text messages when s...</td>\n",
       "      <td>&lt;p&gt;is there a way to prevent the messages app ...</td>\n",
       "      <td>0</td>\n",
       "      <td>836</td>\n",
       "      <td>android</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>49</td>\n",
       "      <td>0.000</td>\n",
       "      <td>['automatically', 'crop', 'text', 'messages', ...</td>\n",
       "      <td>['way', 'prevent', 'messages', 'app', 'sending...</td>\n",
       "      <td>automatically crop text messages sms character...</td>\n",
       "      <td>way prevent messages app sending long texts ge...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>124532</td>\n",
       "      <td>can't find text message that was to a group</td>\n",
       "      <td>&lt;p&gt;when john doe texts to a group that include...</td>\n",
       "      <td>1</td>\n",
       "      <td>28</td>\n",
       "      <td>android</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>41</td>\n",
       "      <td>0.000</td>\n",
       "      <td>[\"can't\", 'find', 'text', 'message', 'group']</td>\n",
       "      <td>['john', 'doe', 'texts', 'group', 'includes', ...</td>\n",
       "      <td>can't find text message group</td>\n",
       "      <td>john doe texts group includes appears notifica...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>193875</td>\n",
       "      <td>can't store contacts on my android phone</td>\n",
       "      <td>&lt;p&gt;i was going through all of my installed app...</td>\n",
       "      <td>0</td>\n",
       "      <td>158</td>\n",
       "      <td>android</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>183</td>\n",
       "      <td>0.000</td>\n",
       "      <td>[\"can't\", 'store', 'contacts', 'android', 'pho...</td>\n",
       "      <td>['going', 'installed', 'applications', 'phone'...</td>\n",
       "      <td>can't store contacts android phone</td>\n",
       "      <td>going installed applications phone ago clear j...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>50332</td>\n",
       "      <td>dropbox on samsung galaxy - where is the setti...</td>\n",
       "      <td>&lt;p&gt;on a sony xperia, the settings button in dr...</td>\n",
       "      <td>1</td>\n",
       "      <td>963</td>\n",
       "      <td>android</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>49</td>\n",
       "      <td>0.000</td>\n",
       "      <td>['dropbox', 'samsung', 'galaxy', 'settings', '...</td>\n",
       "      <td>['sony', 'xperia', 'settings', 'button', 'drop...</td>\n",
       "      <td>dropbox samsung galaxy settings button</td>\n",
       "      <td>sony xperia settings button dropbox top right....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46790</th>\n",
       "      <td>174964</td>\n",
       "      <td>everytime i try to add a device (alcatel) to g...</td>\n",
       "      <td>&lt;p&gt;everytime i try to link my alcatel one touc...</td>\n",
       "      <td>0</td>\n",
       "      <td>1193</td>\n",
       "      <td>android</td>\n",
       "      <td>0</td>\n",
       "      <td>18</td>\n",
       "      <td>46</td>\n",
       "      <td>0.375</td>\n",
       "      <td>['everytime', 'try', 'add', 'device', 'alcatel...</td>\n",
       "      <td>['everytime', 'try', 'link', 'alcatel', 'one',...</td>\n",
       "      <td>everytime try add device alcatel google accoun...</td>\n",
       "      <td>everytime try link alcatel one touch google ac...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46791</th>\n",
       "      <td>101944</td>\n",
       "      <td>how to securely root g2 phone (at&amp;t lge lg-d800)?</td>\n",
       "      <td>&lt;p&gt;i've been looking to root my phone, only al...</td>\n",
       "      <td>4</td>\n",
       "      <td>4972</td>\n",
       "      <td>android</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>153</td>\n",
       "      <td>0.400</td>\n",
       "      <td>['securely', 'root', 'g2', 'phone', 'lge', 'lg...</td>\n",
       "      <td>[\"i've\", 'looking', 'root', 'phone', 'methods'...</td>\n",
       "      <td>securely root g2 phone lge lg-d800</td>\n",
       "      <td>i've looking root phone methods require instal...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46792</th>\n",
       "      <td>194088</td>\n",
       "      <td>is fennec f-droid an official product by mozilla?</td>\n",
       "      <td>&lt;p&gt;is the &lt;a href=\"https://f-droid.org/package...</td>\n",
       "      <td>2</td>\n",
       "      <td>3799</td>\n",
       "      <td>android</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>48</td>\n",
       "      <td>0.000</td>\n",
       "      <td>['fennec', 'f-droid', 'official', 'product', '...</td>\n",
       "      <td>['fennec', 'f-droid', 'browser', 'official', '...</td>\n",
       "      <td>fennec f-droid official product mozilla</td>\n",
       "      <td>fennec f-droid browser official product mozill...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46793</th>\n",
       "      <td>57764</td>\n",
       "      <td>whats the difference between cell phone and da...</td>\n",
       "      <td>&lt;p&gt;is cell phone and data/wifi/lte radiation t...</td>\n",
       "      <td>1</td>\n",
       "      <td>131</td>\n",
       "      <td>android</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>66</td>\n",
       "      <td>0.000</td>\n",
       "      <td>['whats', 'difference', 'cell', 'phone', 'data...</td>\n",
       "      <td>['cell', 'phone', 'data', 'wifi', 'lte', 'radi...</td>\n",
       "      <td>whats difference cell phone data radiation</td>\n",
       "      <td>cell phone data wifi lte radiation thing entir...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46794</th>\n",
       "      <td>9150</td>\n",
       "      <td>“there are no android phones associated with t...</td>\n",
       "      <td>&lt;p&gt;i recently bought an elocity a7 internet ta...</td>\n",
       "      <td>4</td>\n",
       "      <td>4179</td>\n",
       "      <td>android</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>123</td>\n",
       "      <td>0.000</td>\n",
       "      <td>['“', 'android', 'phones', 'associated', 'acco...</td>\n",
       "      <td>['recently', 'bought', 'elocity', 'a7', 'inter...</td>\n",
       "      <td>“ android phones associated account. ”</td>\n",
       "      <td>recently bought elocity a7 internet tablet. i'...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>46795 rows × 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           Id                                              Title  \\\n",
       "0      197234  drop\\stop mobile data connection (non-wifi) by...   \n",
       "1      114800  how to automatically crop text messages when s...   \n",
       "2      124532        can't find text message that was to a group   \n",
       "3      193875           can't store contacts on my android phone   \n",
       "4       50332  dropbox on samsung galaxy - where is the setti...   \n",
       "...       ...                                                ...   \n",
       "46790  174964  everytime i try to add a device (alcatel) to g...   \n",
       "46791  101944  how to securely root g2 phone (at&t lge lg-d800)?   \n",
       "46792  194088  is fennec f-droid an official product by mozilla?   \n",
       "46793   57764  whats the difference between cell phone and da...   \n",
       "46794    9150  “there are no android phones associated with t...   \n",
       "\n",
       "                                                    Body  Score  ViewCount  \\\n",
       "0      <p>can i set android 4.4.2 to drop mobile data...      0         34   \n",
       "1      <p>is there a way to prevent the messages app ...      0        836   \n",
       "2      <p>when john doe texts to a group that include...      1         28   \n",
       "3      <p>i was going through all of my installed app...      0        158   \n",
       "4      <p>on a sony xperia, the settings button in dr...      1        963   \n",
       "...                                                  ...    ...        ...   \n",
       "46790  <p>everytime i try to link my alcatel one touc...      0       1193   \n",
       "46791  <p>i've been looking to root my phone, only al...      4       4972   \n",
       "46792  <p>is the <a href=\"https://f-droid.org/package...      2       3799   \n",
       "46793  <p>is cell phone and data/wifi/lte radiation t...      1        131   \n",
       "46794  <p>i recently bought an elocity a7 internet ta...      4       4179   \n",
       "\n",
       "         Label  LabelNum  Title_Length  Body_Length  Title_Sentiment  \\\n",
       "0      android         0             7           34            0.000   \n",
       "1      android         0            12           49            0.000   \n",
       "2      android         0             9           41            0.000   \n",
       "3      android         0             7          183            0.000   \n",
       "4      android         0            10           49            0.000   \n",
       "...        ...       ...           ...          ...              ...   \n",
       "46790  android         0            18           46            0.375   \n",
       "46791  android         0             9          153            0.400   \n",
       "46792  android         0             8           48            0.000   \n",
       "46793  android         0             9           66            0.000   \n",
       "46794  android         0             9          123            0.000   \n",
       "\n",
       "                                            Title_tokens  \\\n",
       "0      ['drop', 'stop', 'mobile', 'data', 'connection...   \n",
       "1      ['automatically', 'crop', 'text', 'messages', ...   \n",
       "2          [\"can't\", 'find', 'text', 'message', 'group']   \n",
       "3      [\"can't\", 'store', 'contacts', 'android', 'pho...   \n",
       "4      ['dropbox', 'samsung', 'galaxy', 'settings', '...   \n",
       "...                                                  ...   \n",
       "46790  ['everytime', 'try', 'add', 'device', 'alcatel...   \n",
       "46791  ['securely', 'root', 'g2', 'phone', 'lge', 'lg...   \n",
       "46792  ['fennec', 'f-droid', 'official', 'product', '...   \n",
       "46793  ['whats', 'difference', 'cell', 'phone', 'data...   \n",
       "46794  ['“', 'android', 'phones', 'associated', 'acco...   \n",
       "\n",
       "                                             Body_tokens  \\\n",
       "0      ['set', 'android', '4.4.2', 'drop', 'mobile', ...   \n",
       "1      ['way', 'prevent', 'messages', 'app', 'sending...   \n",
       "2      ['john', 'doe', 'texts', 'group', 'includes', ...   \n",
       "3      ['going', 'installed', 'applications', 'phone'...   \n",
       "4      ['sony', 'xperia', 'settings', 'button', 'drop...   \n",
       "...                                                  ...   \n",
       "46790  ['everytime', 'try', 'link', 'alcatel', 'one',...   \n",
       "46791  [\"i've\", 'looking', 'root', 'phone', 'methods'...   \n",
       "46792  ['fennec', 'f-droid', 'browser', 'official', '...   \n",
       "46793  ['cell', 'phone', 'data', 'wifi', 'lte', 'radi...   \n",
       "46794  ['recently', 'bought', 'elocity', 'a7', 'inter...   \n",
       "\n",
       "                                             Title_clean  \\\n",
       "0      drop stop mobile data connection non-wifi cond...   \n",
       "1      automatically crop text messages sms character...   \n",
       "2                          can't find text message group   \n",
       "3                     can't store contacts android phone   \n",
       "4                 dropbox samsung galaxy settings button   \n",
       "...                                                  ...   \n",
       "46790  everytime try add device alcatel google accoun...   \n",
       "46791                 securely root g2 phone lge lg-d800   \n",
       "46792            fennec f-droid official product mozilla   \n",
       "46793         whats difference cell phone data radiation   \n",
       "46794             “ android phones associated account. ”   \n",
       "\n",
       "                                              Body_clean  \n",
       "0      set android 4.4.2 drop mobile data connection ...  \n",
       "1      way prevent messages app sending long texts ge...  \n",
       "2      john doe texts group includes appears notifica...  \n",
       "3      going installed applications phone ago clear j...  \n",
       "4      sony xperia settings button dropbox top right....  \n",
       "...                                                  ...  \n",
       "46790  everytime try link alcatel one touch google ac...  \n",
       "46791  i've looking root phone methods require instal...  \n",
       "46792  fennec f-droid browser official product mozill...  \n",
       "46793  cell phone data wifi lte radiation thing entir...  \n",
       "46794  recently bought elocity a7 internet tablet. i'...  \n",
       "\n",
       "[46795 rows x 14 columns]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "input_data = pd.read_csv(\"training_dataset.csv\")\n",
    "input_data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 1 (1.89 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dell\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\dell\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\dell\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\tf_keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFDistilBertForSequenceClassification: ['vocab_transform.weight', 'vocab_layer_norm.weight', 'vocab_projector.bias', 'vocab_transform.bias', 'vocab_layer_norm.bias']\n",
      "- This IS expected if you are initializing TFDistilBertForSequenceClassification from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFDistilBertForSequenceClassification from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights or buffers of the TF 2.0 model TFDistilBertForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['pre_classifier.weight', 'pre_classifier.bias', 'classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "WARNING:tensorflow:AutoGraph could not transform <function infer_framework at 0x00000197C6F72F80> and will run it as-is.\n",
      "Cause: for/else statement not yet supported\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function infer_framework at 0x00000197C6F72F80> and will run it as-is.\n",
      "Cause: for/else statement not yet supported\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING:tensorflow:From C:\\Users\\dell\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\tensorflow\\python\\autograph\\converters\\directives.py:126: The name tf.nn.fused_batch_norm is deprecated. Please use tf.compat.v1.nn.fused_batch_norm instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\dell\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\tensorflow\\python\\autograph\\converters\\directives.py:126: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\dell\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\tf_keras\\src\\engine\\base_layer_utils.py:384: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "2340/2340 [==============================] - 4656s 2s/step - loss: 1.8699 - mean_squared_error: 1.8699 - val_loss: 1.8762 - val_mean_squared_error: 1.8762\n",
      "Epoch 2/3\n",
      "2340/2340 [==============================] - 5152s 2s/step - loss: 1.7661 - mean_squared_error: 1.7661 - val_loss: 1.8909 - val_mean_squared_error: 1.8909\n",
      "Epoch 3/3\n",
      "2340/2340 [==============================] - 8599s 4s/step - loss: 1.6622 - mean_squared_error: 1.6622 - val_loss: 1.8998 - val_mean_squared_error: 1.8998\n",
      "585/585 [==============================] - 306s 523ms/step - loss: 1.8998 - mean_squared_error: 1.8998\n",
      "Mean Squared Error: 1.8997981548309326\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import DistilBertTokenizer, TFDistilBertForSequenceClassification\n",
    "import tensorflow as tf\n",
    "\n",
    "# Load data\n",
    "data = pd.read_csv(\"training_dataset.csv\")\n",
    "\n",
    "# Tokenize and pad sequences using BERT tokenizer\n",
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "maxlen = 100\n",
    "\n",
    "X_title = data['Title_tokens'].apply(lambda x: eval(x)).values\n",
    "X_body = data['Body_tokens'].apply(lambda x: eval(x)).values\n",
    "X_combined = [' '.join(title + body) for title, body in zip(X_title, X_body)]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_combined, data['Score'].values, test_size=0.2, random_state=42)\n",
    "\n",
    "train_encodings = tokenizer(X_train, truncation=True, padding=True, max_length=maxlen)\n",
    "test_encodings = tokenizer(X_test, truncation=True, padding=True, max_length=maxlen)\n",
    "\n",
    "# Convert lists to TensorFlow Dataset\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((\n",
    "    {key: np.array(value) for key, value in train_encodings.items()},\n",
    "    y_train\n",
    ")).shuffle(len(X_train)).batch(16)\n",
    "\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((\n",
    "    {key: np.array(value) for key, value in test_encodings.items()},\n",
    "    y_test\n",
    ")).batch(16)\n",
    "\n",
    "# Load pre-trained DistilBERT model for sequence classification\n",
    "model = TFDistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased', num_labels=1)\n",
    "\n",
    "# Compile the model\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=5e-5)\n",
    "loss = tf.keras.losses.MeanSquaredError()\n",
    "model.compile(optimizer=optimizer, loss=loss, metrics=['mean_squared_error'])\n",
    "\n",
    "# Fine-tune the model\n",
    "model.fit(train_dataset, epochs=3, validation_data=test_dataset)\n",
    "\n",
    "# Evaluate the model\n",
    "loss, mse = model.evaluate(test_dataset)\n",
    "print(\"Mean Squared Error:\", mse)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 3s 3s/step\n",
      "Predicted Score: [1.0394537]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from transformers import DistilBertTokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# Function to preprocess input text\n",
    "def preprocess_input(text, tokenizer, maxlen):\n",
    "    # Tokenize input text\n",
    "    input_ids = tokenizer.encode(text, add_special_tokens=True, truncation=True, max_length=maxlen)\n",
    "    # Pad sequences\n",
    "    input_ids = pad_sequences([input_ids], maxlen=maxlen, dtype=\"long\", value=0, truncating=\"post\", padding=\"post\")\n",
    "    return np.array(input_ids)\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "\n",
    "# Maximum sequence length\n",
    "maxlen = 100\n",
    "\n",
    "# Prompt user for input text\n",
    "user_input = input(\"Enter your question: \")\n",
    "\n",
    "# Preprocess input text\n",
    "input_ids = preprocess_input(user_input, tokenizer, maxlen)\n",
    "\n",
    "# Make prediction\n",
    "predicted_score = model.predict(input_ids)\n",
    "predicted_score = int(np.round(predicted_score[0][0]))\n",
    "\n",
    "# Display predicted score\n",
    "print(\"Predicted Score:\", predicted_score[0][0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFDistilBertForSequenceClassification: ['vocab_transform.weight', 'vocab_layer_norm.weight', 'vocab_projector.bias', 'vocab_transform.bias', 'vocab_layer_norm.bias']\n",
      "- This IS expected if you are initializing TFDistilBertForSequenceClassification from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFDistilBertForSequenceClassification from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights or buffers of the TF 2.0 model TFDistilBertForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['pre_classifier.weight', 'pre_classifier.bias', 'classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "2340/2340 [==============================] - 4215s 2s/step - loss: 1.9781 - mean_squared_error: 1.9781 - val_loss: 2.0336 - val_mean_squared_error: 2.0336\n",
      "Epoch 2/3\n",
      "2340/2340 [==============================] - 3950s 2s/step - loss: 1.9586 - mean_squared_error: 1.9586 - val_loss: 2.0199 - val_mean_squared_error: 2.0199\n",
      "Epoch 3/3\n",
      "2340/2340 [==============================] - 20144s 9s/step - loss: 1.9542 - mean_squared_error: 1.9542 - val_loss: 1.9955 - val_mean_squared_error: 1.9955\n",
      "585/585 [==============================] - 217s 371ms/step - loss: 1.9955 - mean_squared_error: 1.9955\n",
      "Mean Squared Error 2: 1.9955143928527832\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import DistilBertTokenizer, TFDistilBertForSequenceClassification\n",
    "import tensorflow as tf\n",
    "\n",
    "# Load data\n",
    "data2 = pd.read_csv(\"training_dataset.csv\")\n",
    "\n",
    "# Tokenize and pad sequences using BERT tokenizer\n",
    "tokenizer2 = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "maxlen2 = 100\n",
    "\n",
    "X_title2 = data2['Title_tokens'].apply(lambda x: eval(x)).values\n",
    "X_body2 = data2['Body_tokens'].apply(lambda x: eval(x)).values\n",
    "X_combined2 = [' '.join(title + body) for title, body in zip(X_title2, X_body2)]\n",
    "\n",
    "X_train2, X_test2, y_train2, y_test2 = train_test_split(X_combined2, data2['Score'].values, test_size=0.2, random_state=42)\n",
    "\n",
    "train_encodings2 = tokenizer2(X_train2, truncation=True, padding=True, max_length=maxlen2)\n",
    "test_encodings2 = tokenizer2(X_test2, truncation=True, padding=True, max_length=maxlen2)\n",
    "\n",
    "# Convert lists to TensorFlow Dataset\n",
    "train_dataset2 = tf.data.Dataset.from_tensor_slices((\n",
    "    {key: np.array(value) for key, value in train_encodings2.items()},\n",
    "    y_train2\n",
    ")).shuffle(len(X_train2)).batch(16)\n",
    "\n",
    "test_dataset2 = tf.data.Dataset.from_tensor_slices((\n",
    "    {key: np.array(value) for key, value in test_encodings2.items()},\n",
    "    y_test2\n",
    ")).batch(16)\n",
    "\n",
    "# Load pre-trained DistilBERT model for sequence classification\n",
    "model2 = TFDistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased', num_labels=1)\n",
    "\n",
    "# Modify the model for regression\n",
    "model2.layers[-1].activation = tf.keras.activations.linear\n",
    "\n",
    "# Compile the model\n",
    "optimizer2 = 'adam'  # Use string identifier\n",
    "loss2 = tf.keras.losses.MeanSquaredError()\n",
    "model2.compile(optimizer=optimizer2, loss=loss2, metrics=['mean_squared_error'])\n",
    "\n",
    "# Fine-tune the model\n",
    "model2.fit(train_dataset2, epochs=3, validation_data=test_dataset2)\n",
    "\n",
    "# Evaluate the model\n",
    "loss2, mse2 = model2.evaluate(test_dataset2)\n",
    "print(\"Mean Squared Error 2:\", mse2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model DistilBert ( 1.04 ) The Chosen One"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFDistilBertForSequenceClassification: ['vocab_layer_norm.bias', 'vocab_transform.bias', 'vocab_projector.bias', 'vocab_layer_norm.weight', 'vocab_transform.weight']\n",
      "- This IS expected if you are initializing TFDistilBertForSequenceClassification from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFDistilBertForSequenceClassification from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights or buffers of the TF 2.0 model TFDistilBertForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['pre_classifier.weight', 'pre_classifier.bias', 'classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\dell\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\tf_keras\\src\\optimizers\\__init__.py:317: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "Epoch 1/3\n",
      "WARNING:tensorflow:AutoGraph could not transform <function infer_framework at 0x00000195C1A7B250> and will run it as-is.\n",
      "Cause: for/else statement not yet supported\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function infer_framework at 0x00000195C1A7B250> and will run it as-is.\n",
      "Cause: for/else statement not yet supported\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "2340/2340 [==============================] - 13911s 6s/step - loss: 1.0526 - mean_absolute_error: 1.0526 - val_loss: 1.0527 - val_mean_absolute_error: 1.0527\n",
      "Epoch 2/3\n",
      "2340/2340 [==============================] - 5389s 2s/step - loss: 1.0438 - mean_absolute_error: 1.0438 - val_loss: 1.0644 - val_mean_absolute_error: 1.0644\n",
      "Epoch 3/3\n",
      "2340/2340 [==============================] - 5643s 2s/step - loss: 1.0369 - mean_absolute_error: 1.0369 - val_loss: 1.0493 - val_mean_absolute_error: 1.0493\n",
      "585/585 [==============================] - 285s 487ms/step - loss: 1.0493 - mean_absolute_error: 1.0493\n",
      "Mean Absolute Error 3: 1.0493093729019165\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import DistilBertTokenizer, TFDistilBertForSequenceClassification\n",
    "import tensorflow as tf\n",
    "\n",
    "# Load data\n",
    "data3 = pd.read_csv(\"training_dataset.csv\")\n",
    "\n",
    "# Tokenize and pad sequences using BERT tokenizer\n",
    "tokenizer3 = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "maxlen3 = 100\n",
    "\n",
    "X_title3 = data3['Title_tokens'].apply(lambda x: eval(x)).values\n",
    "X_body3 = data3['Body_tokens'].apply(lambda x: eval(x)).values\n",
    "X_combined3 = [' '.join(title + body) for title, body in zip(X_title3, X_body3)]\n",
    "\n",
    "X_train3, X_test3, y_train3, y_test3 = train_test_split(X_combined3, data3['Score'].values, test_size=0.2, random_state=42)\n",
    "\n",
    "train_encodings3 = tokenizer3(X_train3, truncation=True, padding=True, max_length=maxlen3)\n",
    "test_encodings3 = tokenizer3(X_test3, truncation=True, padding=True, max_length=maxlen3)\n",
    "\n",
    "# Convert lists to TensorFlow Dataset\n",
    "train_dataset3 = tf.data.Dataset.from_tensor_slices((\n",
    "    {key: np.array(value) for key, value in train_encodings3.items()},\n",
    "    y_train3\n",
    ")).shuffle(len(X_train3)).batch(16)\n",
    "\n",
    "test_dataset3 = tf.data.Dataset.from_tensor_slices((\n",
    "    {key: np.array(value) for key, value in test_encodings3.items()},\n",
    "    y_test3\n",
    ")).batch(16)\n",
    "\n",
    "# Load pre-trained DistilBERT model for sequence classification\n",
    "model3 = TFDistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased', num_labels=1)\n",
    "\n",
    "# Modify the model for regression\n",
    "model3.layers[-1].activation = tf.keras.activations.linear\n",
    "\n",
    "# Compile the model with Mean Absolute Error (MAE) as the loss function and evaluation metric\n",
    "optimizer3 = 'adam'  # Use string identifier\n",
    "loss3 = tf.keras.losses.MeanAbsoluteError()  # Change to MAE\n",
    "model3.compile(optimizer=optimizer3, loss=loss3, metrics=['mean_absolute_error'])  # Using MAE as a metric for monitoring\n",
    "\n",
    "# Fine-tune the model\n",
    "model3.fit(train_dataset3, epochs=3, validation_data=test_dataset3)  # Increased epochs\n",
    "\n",
    "# Evaluate the model\n",
    "loss3, mae3 = model3.evaluate(test_dataset3)\n",
    "print(\"Mean Absolute Error 3:\", mae3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Skipping full serialization of TF-Keras layer <tf_keras.src.layers.regularization.dropout.Dropout object at 0x00000195DB30C340>, because it is not built.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Skipping full serialization of TF-Keras layer <tf_keras.src.layers.regularization.dropout.Dropout object at 0x00000195DB30C340>, because it is not built.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Skipping full serialization of TF-Keras layer <tf_keras.src.layers.regularization.dropout.Dropout object at 0x00000195D58A8A30>, because it is not built.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Skipping full serialization of TF-Keras layer <tf_keras.src.layers.regularization.dropout.Dropout object at 0x00000195D58A8A30>, because it is not built.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Skipping full serialization of TF-Keras layer <tf_keras.src.layers.regularization.dropout.Dropout object at 0x00000195DB428FA0>, because it is not built.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Skipping full serialization of TF-Keras layer <tf_keras.src.layers.regularization.dropout.Dropout object at 0x00000195DB428FA0>, because it is not built.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Skipping full serialization of TF-Keras layer <tf_keras.src.layers.regularization.dropout.Dropout object at 0x00000195DB4EFA00>, because it is not built.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Skipping full serialization of TF-Keras layer <tf_keras.src.layers.regularization.dropout.Dropout object at 0x00000195DB4EFA00>, because it is not built.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Skipping full serialization of TF-Keras layer <tf_keras.src.layers.regularization.dropout.Dropout object at 0x00000195DB322020>, because it is not built.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Skipping full serialization of TF-Keras layer <tf_keras.src.layers.regularization.dropout.Dropout object at 0x00000195DB322020>, because it is not built.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Skipping full serialization of TF-Keras layer <tf_keras.src.layers.regularization.dropout.Dropout object at 0x00000195DB433C40>, because it is not built.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Skipping full serialization of TF-Keras layer <tf_keras.src.layers.regularization.dropout.Dropout object at 0x00000195DB433C40>, because it is not built.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: D:/Sem10/NLP/project/SavedModels\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: D:/Sem10/NLP/project/SavedModels\\assets\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "model3.save('D:/Sem10/NLP/project/SavedModels')\n",
    "\n",
    "#loaded_model3 = load_model('D:/Sem10/NLP/project/SavedModels')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 56ms/step\n",
      "Predicted Score: 1\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from transformers import DistilBertTokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# Function to preprocess input text\n",
    "def preprocess_input(text, tokenizer, maxlen):\n",
    "    # Tokenize input text\n",
    "    input_ids = tokenizer.encode(text, add_special_tokens=True, truncation=True, max_length=maxlen)\n",
    "    # Pad sequences\n",
    "    input_ids = pad_sequences([input_ids], maxlen=maxlen, dtype=\"long\", value=0, truncating=\"post\", padding=\"post\")\n",
    "    return np.array(input_ids)\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "\n",
    "# Maximum sequence length\n",
    "maxlen = 100\n",
    "\n",
    "# Prompt user for input text\n",
    "user_input = input(\"Enter your question: \")\n",
    "\n",
    "# Preprocess input text\n",
    "input_ids = preprocess_input(user_input, tokenizer, maxlen)\n",
    "attention_mask = np.ones_like(input_ids)\n",
    "\n",
    "# Convert input to a dictionary to match the model input format\n",
    "input_dict = {\n",
    "    'input_ids': input_ids,\n",
    "    'attention_mask': attention_mask\n",
    "}\n",
    "\n",
    "# Make prediction\n",
    "predicted_score = model3.predict(input_dict)\n",
    "\n",
    "# Convert the predicted score to a natural number\n",
    "predicted_score = int(np.round(predicted_score[0][0]))\n",
    "\n",
    "# Display predicted score\n",
    "print(\"Predicted Score:\", predicted_score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 74ms/step\n",
      "Predicted Score: 1\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from transformers import DistilBertTokenizer\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "# Load the saved model\n",
    "#model3 = load_model('D:/Sem10/NLP/project/SavedModels\\assets')\n",
    "\n",
    "# Load the tokenizer\n",
    "tokenizer3 = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "\n",
    "def predict_score(input_text, model, tokenizer, maxlen=100):\n",
    "    # Tokenize and encode the input text\n",
    "    encoding = tokenizer(input_text, truncation=True, padding='max_length', max_length=maxlen, return_tensors='tf')\n",
    "    \n",
    "    # Get model predictions\n",
    "    predictions = model.predict(encoding)\n",
    "    \n",
    "    # Extract and round the score\n",
    "    score = np.round(predictions[0][0])\n",
    "\n",
    "    return int(score)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Take input from the user\n",
    "    input_text = input(\"Enter the combined title and body: \")\n",
    "\n",
    "    # Predict the score\n",
    "    predicted_score = predict_score(input_text, model3, tokenizer3)\n",
    "\n",
    "    # Output the rounded score\n",
    "    print(f\"Predicted Score: {predicted_score}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Roberta ( 1.05 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dell\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\dell\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\dell\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\tf_keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFRobertaModel: ['roberta.embeddings.position_ids', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.layer_norm.weight']\n",
      "- This IS expected if you are initializing TFRobertaModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFRobertaModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights or buffers of the TF 2.0 model TFRobertaModel were not initialized from the PyTorch model and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\dell\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\tensorflow\\python\\autograph\\converters\\directives.py:126: The name tf.nn.fused_batch_norm is deprecated. Please use tf.compat.v1.nn.fused_batch_norm instead.\n",
      "\n",
      "Epoch 1/3\n",
      "WARNING:tensorflow:From C:\\Users\\dell\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\tensorflow\\python\\autograph\\converters\\directives.py:126: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "WARNING:tensorflow:From C:\\Users\\dell\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\tf_keras\\src\\engine\\base_layer_utils.py:384: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "2340/2340 [==============================] - 11800s 5s/step - loss: 1.0641 - mean_absolute_error: 1.0641 - val_loss: 1.0839 - val_mean_absolute_error: 1.0839\n",
      "Epoch 2/3\n",
      "2340/2340 [==============================] - 12493s 5s/step - loss: 1.0453 - mean_absolute_error: 1.0453 - val_loss: 1.0609 - val_mean_absolute_error: 1.0609\n",
      "Epoch 3/3\n",
      "2340/2340 [==============================] - 11797s 5s/step - loss: 1.0371 - mean_absolute_error: 1.0371 - val_loss: 1.0556 - val_mean_absolute_error: 1.0556\n",
      "585/585 [==============================] - 434s 742ms/step - loss: 1.0556 - mean_absolute_error: 1.0556\n",
      "Mean Absolute Error (RoBERTa): 1.0556235313415527\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import RobertaTokenizer, TFRobertaModel\n",
    "import tensorflow as tf\n",
    "\n",
    "# Load data\n",
    "data4 = pd.read_csv(\"training_dataset.csv\")\n",
    "\n",
    "# Tokenize and pad sequences using RoBERTa tokenizer\n",
    "tokenizer4 = RobertaTokenizer.from_pretrained('roberta-base')\n",
    "maxlen4 = 100\n",
    "\n",
    "X_title4 = data4['Title_tokens'].apply(lambda x: eval(x)).values\n",
    "X_body4 = data4['Body_tokens'].apply(lambda x: eval(x)).values\n",
    "X_combined4 = [' '.join(title + body) for title, body in zip(X_title4, X_body4)]\n",
    "\n",
    "X_train4, X_test4, y_train4, y_test4 = train_test_split(X_combined4, data4['Score'].values, test_size=0.2, random_state=42)\n",
    "\n",
    "train_encodings4 = tokenizer4(X_train4, truncation=True, padding=True, max_length=maxlen4)\n",
    "test_encodings4 = tokenizer4(X_test4, truncation=True, padding=True, max_length=maxlen4)\n",
    "\n",
    "# Convert lists to TensorFlow Dataset\n",
    "train_dataset4 = tf.data.Dataset.from_tensor_slices((\n",
    "    {key: np.array(value) for key, value in train_encodings4.items()},\n",
    "    y_train4\n",
    ")).shuffle(len(X_train4)).batch(16)\n",
    "\n",
    "test_dataset4 = tf.data.Dataset.from_tensor_slices((\n",
    "    {key: np.array(value) for key, value in test_encodings4.items()},\n",
    "    y_test4\n",
    ")).batch(16)\n",
    "\n",
    "# Load pre-trained RoBERTa model\n",
    "roberta_model4 = TFRobertaModel.from_pretrained('roberta-base')\n",
    "\n",
    "# Define input layers\n",
    "input_ids4 = tf.keras.layers.Input(shape=(maxlen4,), dtype=tf.int32, name='input_ids')\n",
    "attention_mask4 = tf.keras.layers.Input(shape=(maxlen4,), dtype=tf.int32, name='attention_mask')\n",
    "\n",
    "# Call RoBERTa model with input ids and attention masks\n",
    "roberta_outputs4 = roberta_model4(input_ids4, attention_mask=attention_mask4)[0]\n",
    "\n",
    "# Take the CLS token representation\n",
    "cls_token4 = roberta_outputs4[:, 0, :]\n",
    "\n",
    "# Define regression head\n",
    "outputs4 = tf.keras.layers.Dense(1, activation='linear')(cls_token4)\n",
    "\n",
    "# Define model\n",
    "model4 = tf.keras.Model(inputs=[input_ids4, attention_mask4], outputs=outputs4)\n",
    "\n",
    "# Compile the model with Mean Absolute Error loss\n",
    "optimizer4 = tf.keras.optimizers.Adam(learning_rate=5e-5)\n",
    "loss4 = tf.keras.losses.MeanAbsoluteError()\n",
    "model4.compile(optimizer=optimizer4, loss=loss4, metrics=['mean_absolute_error'])\n",
    "\n",
    "# Fine-tune the model\n",
    "model4.fit(train_dataset4, epochs=3, validation_data=test_dataset4)\n",
    "\n",
    "# Evaluate the model\n",
    "mae4 = model4.evaluate(test_dataset4)[1]\n",
    "print(\"Mean Absolute Error (RoBERTa):\", mae4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 9s 9s/step\n",
      "Predicted Score: 1\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from transformers import RobertaTokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# Function to preprocess input text\n",
    "def preprocess_input(text, tokenizer, maxlen):\n",
    "    # Tokenize input text\n",
    "    input_ids = tokenizer.encode(text, add_special_tokens=True, truncation=True, max_length=maxlen)\n",
    "    # Pad sequences\n",
    "    input_ids = pad_sequences([input_ids], maxlen=maxlen, dtype=\"long\", value=0, truncating=\"post\", padding=\"post\")\n",
    "    return np.array(input_ids)\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
    "\n",
    "# Maximum sequence length\n",
    "maxlen = 100\n",
    "\n",
    "# Prompt user for input text\n",
    "user_input = input(\"Enter your question: \")\n",
    "\n",
    "# Preprocess input text\n",
    "input_ids = preprocess_input(user_input, tokenizer, maxlen)\n",
    "attention_mask = np.ones_like(input_ids)\n",
    "\n",
    "# Convert input to a dictionary to match the model input format\n",
    "input_dict = {\n",
    "    'input_ids': input_ids,\n",
    "    'attention_mask': attention_mask\n",
    "}\n",
    "\n",
    "# Make prediction\n",
    "predicted_score = model4.predict(input_dict)\n",
    "\n",
    "# Convert the predicted score to a natural number\n",
    "predicted_score = int(np.round(predicted_score[0][0]))\n",
    "\n",
    "# Display predicted score\n",
    "print(\"Predicted Score:\", predicted_score)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Albert ( 1.05 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFAlbertModel: ['predictions.bias', 'predictions.dense.bias', 'predictions.LayerNorm.bias', 'predictions.LayerNorm.weight', 'predictions.decoder.bias', 'predictions.dense.weight']\n",
      "- This IS expected if you are initializing TFAlbertModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFAlbertModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of TFAlbertModel were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFAlbertModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dell\\AppData\\Local\\Temp\\__autograph_generated_file_2exnhdl.py:63: UserWarning: Input dict contained keys ['token_type_ids'] which did not match any model input. They will be ignored by the model.\n",
      "  ag__.converted_call(ag__.ld(warnings).warn, (ag__.converted_call('Input dict contained keys {} which did not match any model input. They will be ignored by the model.'.format, ([ag__.ld(n) for n in ag__.converted_call(ag__.ld(tensors).keys, (), None, fscope) if ag__.ld(n) not in ag__.ld(ref_input_names)],), None, fscope),), dict(stacklevel=2), fscope)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_albert_model/albert/pooler/kernel:0', 'tf_albert_model/albert/pooler/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_albert_model/albert/pooler/kernel:0', 'tf_albert_model/albert/pooler/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_albert_model/albert/pooler/kernel:0', 'tf_albert_model/albert/pooler/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_albert_model/albert/pooler/kernel:0', 'tf_albert_model/albert/pooler/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "2340/2340 [==============================] - 7759s 3s/step - loss: 1.0739 - mean_absolute_error: 1.0739 - val_loss: 1.0891 - val_mean_absolute_error: 1.0891\n",
      "Epoch 2/3\n",
      "2340/2340 [==============================] - 6876s 3s/step - loss: 1.0399 - mean_absolute_error: 1.0399 - val_loss: 1.0481 - val_mean_absolute_error: 1.0481\n",
      "Epoch 3/3\n",
      "2340/2340 [==============================] - 9562s 4s/step - loss: 1.0341 - mean_absolute_error: 1.0341 - val_loss: 1.0563 - val_mean_absolute_error: 1.0563\n",
      "585/585 [==============================] - 425s 726ms/step - loss: 1.0563 - mean_absolute_error: 1.0563\n",
      "Mean Absolute Error (ALBERT): 1.0562832355499268\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import AlbertTokenizer, TFAlbertModel\n",
    "import tensorflow as tf\n",
    "\n",
    "# Load data\n",
    "data5 = pd.read_csv(\"training_dataset.csv\")\n",
    "\n",
    "# Tokenize and pad sequences using ALBERT tokenizer\n",
    "tokenizer5 = AlbertTokenizer.from_pretrained('albert-base-v2')\n",
    "maxlen5 = 100\n",
    "\n",
    "X_title5 = data5['Title_tokens'].apply(lambda x: eval(x)).values\n",
    "X_body5 = data5['Body_tokens'].apply(lambda x: eval(x)).values\n",
    "X_combined5 = [' '.join(title + body) for title, body in zip(X_title5, X_body5)]\n",
    "\n",
    "X_train5, X_test5, y_train5, y_test5 = train_test_split(X_combined5, data5['Score'].values, test_size=0.2, random_state=42)\n",
    "\n",
    "train_encodings5 = tokenizer5(X_train5, truncation=True, padding=True, max_length=maxlen5)\n",
    "test_encodings5 = tokenizer5(X_test5, truncation=True, padding=True, max_length=maxlen5)\n",
    "\n",
    "# Convert lists to TensorFlow Dataset\n",
    "train_dataset5 = tf.data.Dataset.from_tensor_slices((\n",
    "    {key: np.array(value) for key, value in train_encodings5.items()},\n",
    "    y_train5\n",
    ")).shuffle(len(X_train5)).batch(16)\n",
    "\n",
    "test_dataset5 = tf.data.Dataset.from_tensor_slices((\n",
    "    {key: np.array(value) for key, value in test_encodings5.items()},\n",
    "    y_test5\n",
    ")).batch(16)\n",
    "\n",
    "# Load pre-trained ALBERT model\n",
    "albert_model5 = TFAlbertModel.from_pretrained('albert-base-v2')\n",
    "\n",
    "# Define input layers\n",
    "input_ids5 = tf.keras.layers.Input(shape=(maxlen5,), dtype=tf.int32, name='input_ids')\n",
    "attention_mask5 = tf.keras.layers.Input(shape=(maxlen5,), dtype=tf.int32, name='attention_mask')\n",
    "\n",
    "# Call ALBERT model with input ids and attention masks\n",
    "albert_outputs5 = albert_model5(input_ids5, attention_mask=attention_mask5)[0]\n",
    "\n",
    "# Take the CLS token representation\n",
    "cls_token5 = albert_outputs5[:, 0, :]\n",
    "\n",
    "# Define regression head\n",
    "outputs5 = tf.keras.layers.Dense(1, activation='linear')(cls_token5)\n",
    "\n",
    "# Define model\n",
    "model5 = tf.keras.Model(inputs=[input_ids5, attention_mask5], outputs=outputs5)\n",
    "\n",
    "# Compile the model with Mean Absolute Error loss\n",
    "optimizer5 = tf.keras.optimizers.Adam(learning_rate=5e-5)\n",
    "loss5 = tf.keras.losses.MeanAbsoluteError()\n",
    "model5.compile(optimizer=optimizer5, loss=loss5, metrics=['mean_absolute_error'])\n",
    "\n",
    "# Fine-tune the model\n",
    "model5.fit(train_dataset5, epochs=3, validation_data=test_dataset5)\n",
    "\n",
    "# Evaluate the model\n",
    "mae5 = model5.evaluate(test_dataset5)[1]\n",
    "print(\"Mean Absolute Error (ALBERT):\", mae5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model XLNet ( 1.06 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dell\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\dell\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dell\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\huggingface_hub\\file_download.py:148: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\dell\\.cache\\huggingface\\hub\\models--xlnet-base-cased. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\dell\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\tf_keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dell\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\tf_keras\\src\\initializers\\initializers.py:121: UserWarning: The initializer TruncatedNormal is unseeded and being called multiple times, which will return identical values each time (even if the initializer is unseeded). Please update your code to provide a seed to the initializer, or avoid using the same initializer instance more than once.\n",
      "  warnings.warn(\n",
      "Some layers from the model checkpoint at xlnet-base-cased were not used when initializing TFXLNetModel: ['lm_loss']\n",
      "- This IS expected if you are initializing TFXLNetModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFXLNetModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFXLNetModel were initialized from the model checkpoint at xlnet-base-cased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFXLNetModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\dell\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\tensorflow\\python\\autograph\\converters\\directives.py:126: The name tf.nn.fused_batch_norm is deprecated. Please use tf.compat.v1.nn.fused_batch_norm instead.\n",
      "\n",
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dell\\AppData\\Local\\Temp\\__autograph_generated_file_2exnhdl.py:63: UserWarning: Input dict contained keys ['token_type_ids'] which did not match any model input. They will be ignored by the model.\n",
      "  ag__.converted_call(ag__.ld(warnings).warn, (ag__.converted_call('Input dict contained keys {} which did not match any model input. They will be ignored by the model.'.format, ([ag__.ld(n) for n in ag__.converted_call(ag__.ld(tensors).keys, (), None, fscope) if ag__.ld(n) not in ag__.ld(ref_input_names)],), None, fscope),), dict(stacklevel=2), fscope)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\dell\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\tensorflow\\python\\autograph\\converters\\directives.py:126: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tfxl_net_model/transformer/mask_emb:0', 'tfxl_net_model/transformer/layer_._0/rel_attn/r_s_bias:0', 'tfxl_net_model/transformer/layer_._0/rel_attn/seg_embed:0', 'tfxl_net_model/transformer/layer_._1/rel_attn/r_s_bias:0', 'tfxl_net_model/transformer/layer_._1/rel_attn/seg_embed:0', 'tfxl_net_model/transformer/layer_._2/rel_attn/r_s_bias:0', 'tfxl_net_model/transformer/layer_._2/rel_attn/seg_embed:0', 'tfxl_net_model/transformer/layer_._3/rel_attn/r_s_bias:0', 'tfxl_net_model/transformer/layer_._3/rel_attn/seg_embed:0', 'tfxl_net_model/transformer/layer_._4/rel_attn/r_s_bias:0', 'tfxl_net_model/transformer/layer_._4/rel_attn/seg_embed:0', 'tfxl_net_model/transformer/layer_._5/rel_attn/r_s_bias:0', 'tfxl_net_model/transformer/layer_._5/rel_attn/seg_embed:0', 'tfxl_net_model/transformer/layer_._6/rel_attn/r_s_bias:0', 'tfxl_net_model/transformer/layer_._6/rel_attn/seg_embed:0', 'tfxl_net_model/transformer/layer_._7/rel_attn/r_s_bias:0', 'tfxl_net_model/transformer/layer_._7/rel_attn/seg_embed:0', 'tfxl_net_model/transformer/layer_._8/rel_attn/r_s_bias:0', 'tfxl_net_model/transformer/layer_._8/rel_attn/seg_embed:0', 'tfxl_net_model/transformer/layer_._9/rel_attn/r_s_bias:0', 'tfxl_net_model/transformer/layer_._9/rel_attn/seg_embed:0', 'tfxl_net_model/transformer/layer_._10/rel_attn/r_s_bias:0', 'tfxl_net_model/transformer/layer_._10/rel_attn/seg_embed:0', 'tfxl_net_model/transformer/layer_._11/rel_attn/r_s_bias:0', 'tfxl_net_model/transformer/layer_._11/rel_attn/seg_embed:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tfxl_net_model/transformer/mask_emb:0', 'tfxl_net_model/transformer/layer_._0/rel_attn/r_s_bias:0', 'tfxl_net_model/transformer/layer_._0/rel_attn/seg_embed:0', 'tfxl_net_model/transformer/layer_._1/rel_attn/r_s_bias:0', 'tfxl_net_model/transformer/layer_._1/rel_attn/seg_embed:0', 'tfxl_net_model/transformer/layer_._2/rel_attn/r_s_bias:0', 'tfxl_net_model/transformer/layer_._2/rel_attn/seg_embed:0', 'tfxl_net_model/transformer/layer_._3/rel_attn/r_s_bias:0', 'tfxl_net_model/transformer/layer_._3/rel_attn/seg_embed:0', 'tfxl_net_model/transformer/layer_._4/rel_attn/r_s_bias:0', 'tfxl_net_model/transformer/layer_._4/rel_attn/seg_embed:0', 'tfxl_net_model/transformer/layer_._5/rel_attn/r_s_bias:0', 'tfxl_net_model/transformer/layer_._5/rel_attn/seg_embed:0', 'tfxl_net_model/transformer/layer_._6/rel_attn/r_s_bias:0', 'tfxl_net_model/transformer/layer_._6/rel_attn/seg_embed:0', 'tfxl_net_model/transformer/layer_._7/rel_attn/r_s_bias:0', 'tfxl_net_model/transformer/layer_._7/rel_attn/seg_embed:0', 'tfxl_net_model/transformer/layer_._8/rel_attn/r_s_bias:0', 'tfxl_net_model/transformer/layer_._8/rel_attn/seg_embed:0', 'tfxl_net_model/transformer/layer_._9/rel_attn/r_s_bias:0', 'tfxl_net_model/transformer/layer_._9/rel_attn/seg_embed:0', 'tfxl_net_model/transformer/layer_._10/rel_attn/r_s_bias:0', 'tfxl_net_model/transformer/layer_._10/rel_attn/seg_embed:0', 'tfxl_net_model/transformer/layer_._11/rel_attn/r_s_bias:0', 'tfxl_net_model/transformer/layer_._11/rel_attn/seg_embed:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "WARNING:tensorflow:From C:\\Users\\dell\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\tf_keras\\src\\engine\\base_layer_utils.py:384: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tfxl_net_model/transformer/mask_emb:0', 'tfxl_net_model/transformer/layer_._0/rel_attn/r_s_bias:0', 'tfxl_net_model/transformer/layer_._0/rel_attn/seg_embed:0', 'tfxl_net_model/transformer/layer_._1/rel_attn/r_s_bias:0', 'tfxl_net_model/transformer/layer_._1/rel_attn/seg_embed:0', 'tfxl_net_model/transformer/layer_._2/rel_attn/r_s_bias:0', 'tfxl_net_model/transformer/layer_._2/rel_attn/seg_embed:0', 'tfxl_net_model/transformer/layer_._3/rel_attn/r_s_bias:0', 'tfxl_net_model/transformer/layer_._3/rel_attn/seg_embed:0', 'tfxl_net_model/transformer/layer_._4/rel_attn/r_s_bias:0', 'tfxl_net_model/transformer/layer_._4/rel_attn/seg_embed:0', 'tfxl_net_model/transformer/layer_._5/rel_attn/r_s_bias:0', 'tfxl_net_model/transformer/layer_._5/rel_attn/seg_embed:0', 'tfxl_net_model/transformer/layer_._6/rel_attn/r_s_bias:0', 'tfxl_net_model/transformer/layer_._6/rel_attn/seg_embed:0', 'tfxl_net_model/transformer/layer_._7/rel_attn/r_s_bias:0', 'tfxl_net_model/transformer/layer_._7/rel_attn/seg_embed:0', 'tfxl_net_model/transformer/layer_._8/rel_attn/r_s_bias:0', 'tfxl_net_model/transformer/layer_._8/rel_attn/seg_embed:0', 'tfxl_net_model/transformer/layer_._9/rel_attn/r_s_bias:0', 'tfxl_net_model/transformer/layer_._9/rel_attn/seg_embed:0', 'tfxl_net_model/transformer/layer_._10/rel_attn/r_s_bias:0', 'tfxl_net_model/transformer/layer_._10/rel_attn/seg_embed:0', 'tfxl_net_model/transformer/layer_._11/rel_attn/r_s_bias:0', 'tfxl_net_model/transformer/layer_._11/rel_attn/seg_embed:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tfxl_net_model/transformer/mask_emb:0', 'tfxl_net_model/transformer/layer_._0/rel_attn/r_s_bias:0', 'tfxl_net_model/transformer/layer_._0/rel_attn/seg_embed:0', 'tfxl_net_model/transformer/layer_._1/rel_attn/r_s_bias:0', 'tfxl_net_model/transformer/layer_._1/rel_attn/seg_embed:0', 'tfxl_net_model/transformer/layer_._2/rel_attn/r_s_bias:0', 'tfxl_net_model/transformer/layer_._2/rel_attn/seg_embed:0', 'tfxl_net_model/transformer/layer_._3/rel_attn/r_s_bias:0', 'tfxl_net_model/transformer/layer_._3/rel_attn/seg_embed:0', 'tfxl_net_model/transformer/layer_._4/rel_attn/r_s_bias:0', 'tfxl_net_model/transformer/layer_._4/rel_attn/seg_embed:0', 'tfxl_net_model/transformer/layer_._5/rel_attn/r_s_bias:0', 'tfxl_net_model/transformer/layer_._5/rel_attn/seg_embed:0', 'tfxl_net_model/transformer/layer_._6/rel_attn/r_s_bias:0', 'tfxl_net_model/transformer/layer_._6/rel_attn/seg_embed:0', 'tfxl_net_model/transformer/layer_._7/rel_attn/r_s_bias:0', 'tfxl_net_model/transformer/layer_._7/rel_attn/seg_embed:0', 'tfxl_net_model/transformer/layer_._8/rel_attn/r_s_bias:0', 'tfxl_net_model/transformer/layer_._8/rel_attn/seg_embed:0', 'tfxl_net_model/transformer/layer_._9/rel_attn/r_s_bias:0', 'tfxl_net_model/transformer/layer_._9/rel_attn/seg_embed:0', 'tfxl_net_model/transformer/layer_._10/rel_attn/r_s_bias:0', 'tfxl_net_model/transformer/layer_._10/rel_attn/seg_embed:0', 'tfxl_net_model/transformer/layer_._11/rel_attn/r_s_bias:0', 'tfxl_net_model/transformer/layer_._11/rel_attn/seg_embed:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "2340/2340 [==============================] - 11330s 5s/step - loss: 1.1125 - mean_absolute_error: 1.1125 - val_loss: 1.0729 - val_mean_absolute_error: 1.0729\n",
      "Epoch 2/3\n",
      "2340/2340 [==============================] - 12906s 6s/step - loss: 1.0689 - mean_absolute_error: 1.0689 - val_loss: 1.0792 - val_mean_absolute_error: 1.0792\n",
      "Epoch 3/3\n",
      "2340/2340 [==============================] - 14248s 6s/step - loss: 1.0600 - mean_absolute_error: 1.0600 - val_loss: 1.0640 - val_mean_absolute_error: 1.0640\n",
      "585/585 [==============================] - 798s 1s/step - loss: 1.0640 - mean_absolute_error: 1.0640\n",
      "Mean Absolute Error (XLNet): 1.0639591217041016\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import XLNetTokenizer, TFXLNetModel\n",
    "import tensorflow as tf\n",
    "\n",
    "# Load data\n",
    "data6 = pd.read_csv(\"training_dataset.csv\")\n",
    "\n",
    "# Tokenize and pad sequences using XLNet tokenizer\n",
    "tokenizer6 = XLNetTokenizer.from_pretrained('xlnet-base-cased')\n",
    "maxlen6 = 100\n",
    "\n",
    "X_title6 = data6['Title_tokens'].apply(lambda x: eval(x)).values\n",
    "X_body6 = data6['Body_tokens'].apply(lambda x: eval(x)).values\n",
    "X_combined6 = [' '.join(title + body) for title, body in zip(X_title6, X_body6)]\n",
    "\n",
    "X_train6, X_test6, y_train6, y_test6 = train_test_split(X_combined6, data6['Score'].values, test_size=0.2, random_state=42)\n",
    "\n",
    "train_encodings6 = tokenizer6(X_train6, truncation=True, padding=True, max_length=maxlen6)\n",
    "test_encodings6 = tokenizer6(X_test6, truncation=True, padding=True, max_length=maxlen6)\n",
    "\n",
    "# Convert lists to TensorFlow Dataset\n",
    "train_dataset6 = tf.data.Dataset.from_tensor_slices((\n",
    "    {key: np.array(value) for key, value in train_encodings6.items()},\n",
    "    y_train6\n",
    ")).shuffle(len(X_train6)).batch(16)\n",
    "\n",
    "test_dataset6 = tf.data.Dataset.from_tensor_slices((\n",
    "    {key: np.array(value) for key, value in test_encodings6.items()},\n",
    "    y_test6\n",
    ")).batch(16)\n",
    "\n",
    "# Load pre-trained XLNet model\n",
    "xlnet_model6 = TFXLNetModel.from_pretrained('xlnet-base-cased')\n",
    "\n",
    "# Define input layers\n",
    "input_ids6 = tf.keras.layers.Input(shape=(maxlen6,), dtype=tf.int32, name='input_ids')\n",
    "attention_mask6 = tf.keras.layers.Input(shape=(maxlen6,), dtype=tf.int32, name='attention_mask')\n",
    "\n",
    "# Call XLNet model with input ids and attention masks\n",
    "xlnet_outputs6 = xlnet_model6(input_ids6, attention_mask=attention_mask6)[0]\n",
    "\n",
    "# Take the CLS token representation\n",
    "cls_token6 = xlnet_outputs6[:, 0, :]\n",
    "\n",
    "# Define regression head\n",
    "outputs6 = tf.keras.layers.Dense(1, activation='linear')(cls_token6)\n",
    "\n",
    "# Define model\n",
    "model6 = tf.keras.Model(inputs=[input_ids6, attention_mask6], outputs=outputs6)\n",
    "\n",
    "# Compile the model with Mean Absolute Error loss\n",
    "optimizer6 = tf.keras.optimizers.Adam(learning_rate=5e-5)\n",
    "loss6 = tf.keras.losses.MeanAbsoluteError()\n",
    "model6.compile(optimizer=optimizer6, loss=loss6, metrics=['mean_absolute_error'])\n",
    "\n",
    "# Fine-tune the model\n",
    "model6.fit(train_dataset6, epochs=3, validation_data=test_dataset6)\n",
    "\n",
    "# Evaluate the model\n",
    "mae6 = model6.evaluate(test_dataset6)[1]\n",
    "print(\"Mean Absolute Error (XLNet):\", mae6)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
