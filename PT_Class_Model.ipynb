{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DistilBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFDistilBertForSequenceClassification: ['vocab_layer_norm.bias', 'vocab_layer_norm.weight', 'vocab_transform.bias', 'vocab_projector.bias', 'vocab_transform.weight']\n",
      "- This IS expected if you are initializing TFDistilBertForSequenceClassification from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFDistilBertForSequenceClassification from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights or buffers of the TF 2.0 model TFDistilBertForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['pre_classifier.weight', 'pre_classifier.bias', 'classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "2340/2340 [==============================] - 14438s 6s/step - loss: 0.0658 - accuracy: 0.9788 - val_loss: 0.0600 - val_accuracy: 0.9804\n",
      "Epoch 2/3\n",
      "2340/2340 [==============================] - 14581s 6s/step - loss: 0.0370 - accuracy: 0.9877 - val_loss: 0.0468 - val_accuracy: 0.9845\n",
      "Epoch 3/3\n",
      "2340/2340 [==============================] - 14343s 6s/step - loss: 0.0244 - accuracy: 0.9924 - val_loss: 0.0519 - val_accuracy: 0.9856\n",
      "585/585 [==============================] - 1002s 2s/step - loss: 0.0519 - accuracy: 0.9856\n",
      "Test Accuracy (DistilBERT): 0.9855753779411316\n"
     ]
    }
   ],
   "source": [
    "from transformers import DistilBertTokenizer, TFDistilBertForSequenceClassification\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load data\n",
    "data_distilbert = pd.read_csv(\"training_dataset.csv\")\n",
    "\n",
    "# Tokenize and pad sequences using DistilBERT tokenizer\n",
    "tokenizer_distilbert = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "maxlen_distilbert = 300\n",
    "\n",
    "X_title_distilbert = data_distilbert['Title_tokens'].apply(lambda x: eval(x)).values\n",
    "X_body_distilbert = data_distilbert['Body_tokens'].apply(lambda x: eval(x)).values\n",
    "X_combined_distilbert = [' '.join(title + body) for title, body in zip(X_title_distilbert, X_body_distilbert)]\n",
    "\n",
    "X_train_distilbert, X_test_distilbert, y_train_distilbert, y_test_distilbert = train_test_split(X_combined_distilbert, data_distilbert['LabelNum'].values, test_size=0.2, random_state=42)\n",
    "\n",
    "# Tokenize and encode sequences\n",
    "train_encodings_distilbert = tokenizer_distilbert(X_train_distilbert, truncation=True, padding=True, max_length=maxlen_distilbert)\n",
    "test_encodings_distilbert = tokenizer_distilbert(X_test_distilbert, truncation=True, padding=True, max_length=maxlen_distilbert)\n",
    "\n",
    "# Convert lists to TensorFlow Dataset\n",
    "train_dataset_distilbert = tf.data.Dataset.from_tensor_slices((\n",
    "    {key: np.array(value) for key, value in train_encodings_distilbert.items()},\n",
    "    y_train_distilbert\n",
    ")).shuffle(len(X_train_distilbert)).batch(16)\n",
    "\n",
    "test_dataset_distilbert = tf.data.Dataset.from_tensor_slices((\n",
    "    {key: np.array(value) for key, value in test_encodings_distilbert.items()},\n",
    "    y_test_distilbert\n",
    ")).batch(16)\n",
    "\n",
    "# Load pre-trained DistilBERT model for sequence classification\n",
    "model_distilbert = TFDistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased', num_labels=2)\n",
    "\n",
    "# Compile the model\n",
    "optimizer_distilbert = tf.keras.optimizers.Adam(learning_rate=5e-5)\n",
    "loss_distilbert = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "model_distilbert.compile(optimizer=optimizer_distilbert, loss=loss_distilbert, metrics=['accuracy'])\n",
    "\n",
    "# Add early stopping to mitigate overfitting\n",
    "early_stopping_distilbert = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor='val_loss', patience=3, restore_best_weights=True\n",
    ")\n",
    "\n",
    "# Fit the model\n",
    "history_distilbert = model_distilbert.fit(\n",
    "    train_dataset_distilbert,\n",
    "    epochs=3,\n",
    "    validation_data=test_dataset_distilbert,\n",
    "    callbacks=[early_stopping_distilbert]\n",
    ")\n",
    "\n",
    "# Evaluate the model\n",
    "loss_distilbert, accuracy_distilbert = model_distilbert.evaluate(test_dataset_distilbert)\n",
    "print(\"Test Accuracy (DistilBERT):\", accuracy_distilbert)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess user input\n",
    "user_input = input(\"Enter your text: \")\n",
    "user_input_encoding = tokenizer_distilbert(user_input, truncation=True, padding=True, max_length=maxlen_distilbert, return_tensors=\"tf\")\n",
    "\n",
    "# Pass input to the model\n",
    "predictions = model_distilbert.predict(user_input_encoding)\n",
    "\n",
    "# Interpret model output\n",
    "print(predictions.logits[0][0])\n",
    "print(predictions.logits[0][1])\n",
    "\n",
    "\n",
    "if predictions.logits[0][0] > predictions.logits[0][1]:\n",
    "    predicted_label_st = \"Android\"\n",
    "    print(\"Predicted class:\", predicted_label_st)\n",
    "else:\n",
    "    predicted_label_st = \"IOS\"\n",
    "    print(\"Predicted class:\", predicted_label_st)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "predictions = model_distilbert.predict(test_dataset_distilbert)\n",
    "y_pred = (predictions > 0.5).astype(int)\n",
    "\n",
    "cm = confusion_matrix(y_test_distilbert, y_pred)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False)\n",
    "plt.xlabel('Predicted labels')\n",
    "plt.ylabel('True labels')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()\n",
    "\n",
    "# Calculate classification report\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test_distilbert, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RoBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFRobertaForSequenceClassification: ['roberta.embeddings.position_ids']\n",
      "- This IS expected if you are initializing TFRobertaForSequenceClassification from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFRobertaForSequenceClassification from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights or buffers of the TF 2.0 model TFRobertaForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2340/2340 [==============================] - 29951s 13s/step - loss: 0.0867 - accuracy: 0.9709 - val_loss: 0.0564 - val_accuracy: 0.9837\n",
      "585/585 [==============================] - 2127s 4s/step - loss: 0.0564 - accuracy: 0.9837\n",
      "Test Accuracy (RoBERTa): 0.9836521148681641\n"
     ]
    }
   ],
   "source": [
    "from transformers import RobertaTokenizer, TFRobertaForSequenceClassification\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load data\n",
    "data_roberta = pd.read_csv(\"training_dataset.csv\")\n",
    "\n",
    "# Tokenize and pad sequences using RoBERTa tokenizer\n",
    "tokenizer_roberta = RobertaTokenizer.from_pretrained('roberta-base')\n",
    "maxlen_roberta = 300\n",
    "\n",
    "X_title_roberta = data_roberta['Title_tokens'].apply(lambda x: eval(x)).values\n",
    "X_body_roberta = data_roberta['Body_tokens'].apply(lambda x: eval(x)).values\n",
    "X_combined_roberta = [' '.join(title + body) for title, body in zip(X_title_roberta, X_body_roberta)]\n",
    "\n",
    "X_train_roberta, X_test_roberta, y_train_roberta, y_test_roberta = train_test_split(X_combined_roberta, data_roberta['LabelNum'].values, test_size=0.2, random_state=42)\n",
    "\n",
    "# Tokenize and encode sequences\n",
    "train_encodings_roberta = tokenizer_roberta(X_train_roberta, truncation=True, padding=True, max_length=maxlen_roberta)\n",
    "test_encodings_roberta = tokenizer_roberta(X_test_roberta, truncation=True, padding=True, max_length=maxlen_roberta)\n",
    "\n",
    "# Convert lists to TensorFlow Dataset\n",
    "train_dataset_roberta = tf.data.Dataset.from_tensor_slices((\n",
    "    {key: np.array(value) for key, value in train_encodings_roberta.items()},\n",
    "    y_train_roberta\n",
    ")).shuffle(len(X_train_roberta)).batch(16)\n",
    "\n",
    "test_dataset_roberta = tf.data.Dataset.from_tensor_slices((\n",
    "    {key: np.array(value) for key, value in test_encodings_roberta.items()},\n",
    "    y_test_roberta\n",
    ")).batch(16)\n",
    "\n",
    "# Load pre-trained RoBERTa model for sequence classification\n",
    "model_roberta = TFRobertaForSequenceClassification.from_pretrained('roberta-base', num_labels=2)\n",
    "\n",
    "# Compile the model\n",
    "optimizer_roberta = tf.keras.optimizers.Adam(learning_rate=5e-5)\n",
    "loss_roberta = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "model_roberta.compile(optimizer=optimizer_roberta, loss=loss_roberta, metrics=['accuracy'])\n",
    "\n",
    "# Add early stopping to mitigate overfitting\n",
    "early_stopping_roberta = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor='val_loss', patience=3, restore_best_weights=True\n",
    ")\n",
    "\n",
    "# Fit the model\n",
    "history_roberta = model_roberta.fit(\n",
    "    train_dataset_roberta,\n",
    "    epochs=1,\n",
    "    validation_data=test_dataset_roberta,\n",
    "    callbacks=[early_stopping_roberta]\n",
    ")\n",
    "\n",
    "# Evaluate the model\n",
    "loss_roberta, accuracy_roberta = model_roberta.evaluate(test_dataset_roberta)\n",
    "print(\"Test Accuracy (RoBERTa):\", accuracy_roberta)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess user input\n",
    "user_input = input(\"Enter your text: \")\n",
    "user_input_encoding = tokenizer_roberta(user_input, truncation=True, padding=True, max_length=maxlen_roberta, return_tensors=\"tf\")\n",
    "\n",
    "# Pass input to the model\n",
    "predictions = model_roberta.predict(user_input_encoding)\n",
    "\n",
    "# Interpret model output\n",
    "print(predictions.logits[0][0])\n",
    "print(predictions.logits[0][1])\n",
    "\n",
    "\n",
    "if predictions.logits[0][0] > predictions.logits[0][1]:\n",
    "    predicted_label_st = \"Android\"\n",
    "    print(\"Predicted class:\", predicted_label_st)\n",
    "else:\n",
    "    predicted_label_st = \"IOS\"\n",
    "    print(\"Predicted class:\", predicted_label_st)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "448/585 [=====================>........] - ETA: 8:28"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "predictions = model_roberta.predict(test_dataset_roberta)\n",
    "y_pred = (predictions > 0.5).astype(int)\n",
    "\n",
    "cm = confusion_matrix(y_test_distilbert, y_pred)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False)\n",
    "plt.xlabel('Predicted labels')\n",
    "plt.ylabel('True labels')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()\n",
    "\n",
    "# Calculate classification report\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test_roberta, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: 2\n"
     ]
    }
   ],
   "source": [
    "my_dict = {\n",
    "    \"x\": \"John\",\n",
    "    \"x\": 'ss',\n",
    "    \"x\": 2,\n",
    "    \"city\": \"New York\"\n",
    "}\n",
    "\n",
    "print(\"Name:\", my_dict[\"x\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The sum is:  30\n"
     ]
    }
   ],
   "source": [
    "a = input(\"Write the first number: \" )\n",
    "b = input(\"Write the second number: \" )\n",
    "\n",
    "a = int(a)\n",
    "b = int(b)\n",
    "\n",
    "sum = a+b\n",
    "\n",
    "print(\"The sum is: \" , sum)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
